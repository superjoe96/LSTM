{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt4L5YudZLC1Y6eIezkzDt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superjoe96/LSTM/blob/main/lstm_odl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_0hb8wJZpuia"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('personal_transactions_10000.csv')"
      ],
      "metadata": {
        "id": "aYs9X5PdqAXP"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Preprocessing\n",
        "# Convert Date to datetime if it's not already\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "Cz4rA4JLqPZo"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced feature engineering\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "# Create a more stable time key for sorting\n",
        "df['YearWeek'] = df['Year'].astype(str) + '-' + df['WeekOfYear'].astype(str).str.zfill(2)"
      ],
      "metadata": {
        "id": "RcP2Xb6DwFDJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate credits and debits for better modeling\n",
        "weekly_debits = df[df['Transaction Type'] == 'debit'].groupby(['YearWeek', 'Category'])['Amount'].agg(['sum', 'count', 'mean']).reset_index()\n",
        "weekly_debits['sum'] = weekly_debits['sum'].abs()  # Convert negative amounts to positive\n",
        "weekly_debits['mean'] = weekly_debits['mean'].abs()  # Convert negative means to positive"
      ],
      "metadata": {
        "id": "pUH03OggwFvA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot the data to get each category as columns with different aggregations\n",
        "weekly_sum = weekly_debits.pivot_table(index='YearWeek', columns='Category', values='sum', fill_value=0)\n",
        "weekly_count = weekly_debits.pivot_table(index='YearWeek', columns='Category', values='count', fill_value=0)\n",
        "weekly_mean = weekly_debits.pivot_table(index='YearWeek', columns='Category', values='mean', fill_value=0)\n",
        "\n",
        "# Add prefixes to differentiate the feature types\n",
        "weekly_count.columns = [f'count_{col}' for col in weekly_count.columns]\n",
        "weekly_mean.columns = [f'mean_{col}' for col in weekly_mean.columns]\n",
        "\n",
        "# Merge all features\n",
        "weekly_features = weekly_sum.join(weekly_count).join(weekly_mean)\n",
        "\n",
        "# Reset index and create proper sorting key\n",
        "weekly_features = weekly_features.reset_index()\n",
        "weekly_features['sort_key'] = weekly_features['YearWeek'].apply(\n",
        "    lambda x: int(x.split('-')[0]) * 100 + int(x.split('-')[1]))\n",
        "weekly_features = weekly_features.sort_values('sort_key')\n",
        "weekly_features = weekly_features.drop('sort_key', axis=1)\n",
        "\n",
        "# Add lagged features (previous 1, 2, and 4 weeks)\n",
        "numeric_columns = weekly_features.columns[1:]  # Exclude YearWeek column\n",
        "\n",
        "for col in numeric_columns:\n",
        "    for lag in [1, 2, 4]:\n",
        "        weekly_features[f'{col}_lag{lag}'] = weekly_features[col].shift(lag)\n",
        "\n",
        "# Calculate trending features (percentage changes)\n",
        "for col in numeric_columns:\n",
        "    weekly_features[f'{col}_pct_change'] = weekly_features[col].pct_change()\n",
        "\n",
        "# Replace infinities and NaNs\n",
        "weekly_features = weekly_features.replace([np.inf, -np.inf], np.nan)\n",
        "weekly_features = weekly_features.fillna(0)"
      ],
      "metadata": {
        "id": "jYl3bkLmqTq1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length, forecast_horizon=1):\n",
        "    \"\"\"Create sequences with multi-step forecasting capability\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length:i+seq_length+forecast_horizon])\n",
        "    return np.array(X), np.array(y).reshape(len(y), -1)"
      ],
      "metadata": {
        "id": "RYNeHQQw0djH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numeric data only (excluding the YearWeek column)\n",
        "data = weekly_features.iloc[:, 1:].values\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Define sequence parameters\n",
        "seq_length = 12  # 8 weeks (2 months) - adjusted based on seasonality analysis\n",
        "forecast_horizon = 1  # Predict next week\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(scaled_data, seq_length, forecast_horizon)\n",
        "\n",
        "# This helps detect overfitting early\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)"
      ],
      "metadata": {
        "id": "9YvV7iKh0eHM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. IMPROVED MODEL ARCHITECTURE\n",
        "def build_advanced_lstm_model(input_shape, output_size):\n",
        "    \"\"\"Build a more sophisticated LSTM model with bidirectional layers\"\"\"\n",
        "    model = Sequential([\n",
        "        # Bidirectional LSTM for capturing patterns in both directions\n",
        "        Bidirectional(LSTM(64, activation='tanh', return_sequences=True,\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                     input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Second Bidirectional LSTM layer\n",
        "        Bidirectional(LSTM(32, activation='tanh')),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Dense hidden layer\n",
        "        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(output_size)\n",
        "    ])\n",
        "\n",
        "    # Compile with Huber loss - more robust to outliers than MSE\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),  # Lower learning rate for stability\n",
        "        loss=tf.keras.losses.Huber(),  # Huber loss is less sensitive to outliers\n",
        "        metrics=['mean_absolute_error']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "lCUfwTfv0ktj"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the improved model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # (seq_length, num_features)\n",
        "output_size = y_train.shape[1]  # Size of the output\n",
        "lstm_model = build_advanced_lstm_model(input_shape, output_size)"
      ],
      "metadata": {
        "id": "d4NIG5nj0lRj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train the model\n",
        "# Define callbacks for training\n",
        "callbacks = [\n",
        "    # More patient early stopping\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,  # Wait longer before stopping\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Learning rate scheduler with better parameters\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,  # Reduce LR by more when plateau is detected\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Save the best model\n",
        "    ModelCheckpoint(\n",
        "        'lstm_cashflow_model_improved.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train with a larger batch size and more epochs\n",
        "print(\"\\nTraining the improved model...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # More epochs with early stopping\n",
        "    batch_size=32,  # Smaller batch size for better generalization\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        "    shuffle=False  # Important for time series data\n",
        ")"
      ],
      "metadata": {
        "id": "O31jJgAF0w1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_mae = lstm_model.evaluate(X_train, y_train, verbose=0)\n",
        "val_loss, val_mae = lstm_model.evaluate(X_val, y_val, verbose=0)\n",
        "test_loss, test_mae = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"\\nModel Performance Metrics:\")\n",
        "print(f\"Training Loss: {train_loss:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")"
      ],
      "metadata": {
        "id": "5eLUEOMu005K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions = lstm_model.predict(X_train, verbose=0)\n",
        "val_predictions = lstm_model.predict(X_val, verbose=0)\n",
        "test_predictions = lstm_model.predict(X_test, verbose=0)\n",
        "\n",
        "# Convert back to original scale\n",
        "y_train_orig = scaler.inverse_transform(y_train.reshape(y_train.shape[0], -1))\n",
        "y_val_orig = scaler.inverse_transform(y_val.reshape(y_val.shape[0], -1))\n",
        "y_test_orig = scaler.inverse_transform(y_test.reshape(y_test.shape[0], -1))\n",
        "\n",
        "train_pred_orig = scaler.inverse_transform(train_predictions)\n",
        "val_pred_orig = scaler.inverse_transform(val_predictions)\n",
        "test_pred_orig = scaler.inverse_transform(test_predictions)\n",
        "\n",
        "# Calculate metrics on the original scale\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train_orig, train_pred_orig))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val_orig, val_pred_orig))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test_orig, test_pred_orig))\n",
        "\n",
        "train_r2 = r2_score(y_train_orig, train_pred_orig)\n",
        "val_r2 = r2_score(y_val_orig, val_pred_orig)\n",
        "test_r2 = r2_score(y_test_orig, test_pred_orig)\n",
        "\n",
        "print(\"\\nModel Performance (original scale):\")\n",
        "print(f\"Training RMSE: ${train_rmse:.2f}\")\n",
        "print(f\"Validation RMSE: ${val_rmse:.2f}\")\n",
        "print(f\"Test RMSE: ${test_rmse:.2f}\")\n",
        "print(f\"Training R²: {train_r2:.4f}\")\n",
        "print(f\"Validation R²: {val_r2:.4f}\")\n",
        "print(f\"Test R²: {test_r2:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gs_4cYWe05sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss', fontsize=15)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "plt.title('Model MAE', fontsize=15)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('MAE', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('lstm_training_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jaaaX6u-074g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h86-p402wAN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. COMPREHENSIVE VISUALIZATION OF MODEL PERFORMANCE\n",
        "\n",
        "# Extract sum columns only (no count or mean) for clarity\n",
        "category_columns = [col for col in weekly_features.columns[1:] if not col.startswith('count_')\n",
        "                   and not col.startswith('mean_') and not '_lag' in col and not '_pct' in col]\n",
        "\n",
        "print(\"\\nVisualizing model performance on training and test data...\")\n",
        "\n",
        "# Get top categories by total spending across all datasets\n",
        "all_actual_values = np.vstack([y_train_orig[:, :len(category_columns)],\n",
        "                              y_val_orig[:, :len(category_columns)],\n",
        "                              y_test_orig[:, :len(category_columns)]])\n",
        "category_totals = all_actual_values.sum(axis=0)\n",
        "top_categories = 5\n",
        "top_indices = np.argsort(category_totals)[-top_categories:]\n",
        "\n",
        "# Extract actual and predicted values for training and test sets\n",
        "train_actual = y_train_orig[:, :len(category_columns)]\n",
        "train_predicted = train_pred_orig[:, :len(category_columns)]\n",
        "test_actual = y_test_orig[:, :len(category_columns)]\n",
        "test_predicted = test_pred_orig[:, :len(category_columns)]\n",
        "\n",
        "# 7.1 TRAINING SET VISUALIZATION\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.suptitle('Model Performance on Training Dataset', fontsize=16)\n",
        "\n",
        "for i, idx in enumerate(top_indices):\n",
        "    plt.subplot(top_categories, 1, i+1)\n",
        "\n",
        "    # Plot training data\n",
        "    weeks = range(len(train_actual[:, idx]))\n",
        "    plt.plot(weeks, train_actual[:, idx], 'b-', linewidth=2, label='Actual')\n",
        "    plt.plot(weeks, train_predicted[:, idx], 'r--', linewidth=2, label='Predicted')\n",
        "\n",
        "    # Calculate metrics for this category on training data\n",
        "    cat_mae = mean_absolute_error(train_actual[:, idx], train_predicted[:, idx])\n",
        "    cat_rmse = np.sqrt(mean_squared_error(train_actual[:, idx], train_predicted[:, idx]))\n",
        "    cat_r2 = r2_score(train_actual[:, idx], train_predicted[:, idx])\n",
        "\n",
        "    plt.title(f'Category: {category_columns[idx]} - Training Data', fontsize=14)\n",
        "    plt.ylabel('Amount ($)', fontsize=12)\n",
        "    plt.legend([f'Actual', f'Predicted (MAE: ${cat_mae:.2f}, R²: {cat_r2:.3f})'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add error bars for selected points (every 5th point to avoid clutter)\n",
        "    for j in range(0, len(weeks), 5):\n",
        "        error = abs(train_actual[:, idx][j] - train_predicted[:, idx][j])\n",
        "        plt.errorbar(weeks[j], train_predicted[:, idx][j], yerr=error, fmt='o', color='gray', alpha=0.5)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
        "plt.savefig('training_set_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 7.2 TEST SET VISUALIZATION\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.suptitle('Model Performance on Test Dataset', fontsize=16)\n",
        "\n",
        "for i, idx in enumerate(top_indices):\n",
        "    plt.subplot(top_categories, 1, i+1)\n",
        "\n",
        "    # Plot test data\n",
        "    weeks = range(len(test_actual[:, idx]))\n",
        "    plt.plot(weeks, test_actual[:, idx], 'b-', linewidth=2, label='Actual')\n",
        "    plt.plot(weeks, test_predicted[:, idx], 'r--', linewidth=2, label='Predicted')\n",
        "\n",
        "    # Calculate metrics for this category on test data\n",
        "    cat_mae = mean_absolute_error(test_actual[:, idx], test_predicted[:, idx])\n",
        "    cat_rmse = np.sqrt(mean_squared_error(test_actual[:, idx], test_predicted[:, idx]))\n",
        "    cat_r2 = r2_score(test_actual[:, idx], test_predicted[:, idx])\n",
        "\n",
        "    plt.title(f'Category: {category_columns[idx]} - Test Data', fontsize=14)\n",
        "    plt.ylabel('Amount ($)', fontsize=12)\n",
        "    plt.legend([f'Actual', f'Predicted (MAE: ${cat_mae:.2f}, R²: {cat_r2:.3f})'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add error bars for all points in test set (typically smaller than training set)\n",
        "    for j in range(len(weeks)):\n",
        "        error = abs(test_actual[:, idx][j] - test_predicted[:, idx][j])\n",
        "        plt.errorbar(weeks[j], test_predicted[:, idx][j], yerr=error, fmt='o', color='gray', alpha=0.5)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
        "plt.savefig('test_set_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 7.3 DIRECT COMPARISON OF TRAIN AND TEST PERFORMANCE\n",
        "plt.figure(figsize=(18, 15))\n",
        "plt.suptitle('Comparing Model Performance: Training vs Test Sets', fontsize=16)\n",
        "\n",
        "for i, idx in enumerate(top_indices):\n",
        "    plt.subplot(top_categories, 2, i*2+1)\n",
        "\n",
        "    # Training data comparison\n",
        "    train_weeks = range(len(train_actual[:, idx]))\n",
        "    plt.plot(train_actual[:, idx], 'b-', linewidth=2, label='Actual')\n",
        "    plt.plot(train_predicted[:, idx], 'r--', linewidth=2, label='Predicted')\n",
        "    plt.title(f'{category_columns[idx]} - Training', fontsize=14)\n",
        "    plt.ylabel('Amount ($)', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show distribution of errors\n",
        "    errors = train_predicted[:, idx] - train_actual[:, idx]\n",
        "    ax2 = plt.twinx()\n",
        "    ax2.plot(errors, 'g.', alpha=0.5, label='Error')\n",
        "    ax2.set_ylabel('Error ($)', color='g')\n",
        "    ax2.tick_params(axis='y', labelcolor='g')\n",
        "\n",
        "    plt.legend(['Actual', 'Predicted', 'Error'])\n",
        "\n",
        "    plt.subplot(top_categories, 2, i*2+2)\n",
        "\n",
        "    # Test data comparison\n",
        "    test_weeks = range(len(test_actual[:, idx]))\n",
        "    plt.plot(test_actual[:, idx], 'b-', linewidth=2, label='Actual')\n",
        "    plt.plot(test_predicted[:, idx], 'r--', linewidth=2, label='Predicted')\n",
        "    plt.title(f'{category_columns[idx]} - Test', fontsize=14)\n",
        "    plt.ylabel('Amount ($)', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show distribution of errors\n",
        "    errors = test_predicted[:, idx] - test_actual[:, idx]\n",
        "    ax2 = plt.twinx()\n",
        "    ax2.plot(errors, 'g.', alpha=0.5, label='Error')\n",
        "    ax2.set_ylabel('Error ($)', color='g')\n",
        "    ax2.tick_params(axis='y', labelcolor='g')\n",
        "\n",
        "    plt.legend(['Actual', 'Predicted', 'Error'])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
        "plt.savefig('train_test_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 7.4 ERROR DISTRIBUTION ANALYSIS\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.suptitle('Error Distribution Analysis', fontsize=16)\n",
        "\n",
        "for i, idx in enumerate(top_indices):\n",
        "    plt.subplot(3, 2, i+1)\n",
        "\n",
        "    # Calculate errors\n",
        "    train_errors = train_predicted[:, idx] - train_actual[:, idx]\n",
        "    test_errors = test_predicted[:, idx] - test_actual[:, idx]\n",
        "\n",
        "    # Plot error distributions as histograms\n",
        "    plt.hist(train_errors, bins=20, alpha=0.5, label='Training Errors')\n",
        "    plt.hist(test_errors, bins=20, alpha=0.5, label='Test Errors')\n",
        "\n",
        "    plt.title(f'Error Distribution - {category_columns[idx]}', fontsize=12)\n",
        "    plt.xlabel('Prediction Error ($)', fontsize=10)\n",
        "    plt.ylabel('Frequency', fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "    # Add statistics\n",
        "    train_mean_error = np.mean(train_errors)\n",
        "    test_mean_error = np.mean(test_errors)\n",
        "    train_std_error = np.std(train_errors)\n",
        "    test_std_error = np.std(test_errors)\n",
        "\n",
        "    plt.annotate(f'Train μ={train_mean_error:.2f}, σ={train_std_error:.2f}\\n'\n",
        "                f'Test μ={test_mean_error:.2f}, σ={test_std_error:.2f}',\n",
        "                xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
        "\n",
        "# Add a subplot to show overall error distribution\n",
        "plt.subplot(3, 2, 6)\n",
        "all_train_errors = train_predicted.flatten() - train_actual.flatten()\n",
        "all_test_errors = test_predicted.flatten() - test_actual.flatten()\n",
        "\n",
        "plt.hist(all_train_errors, bins=30, alpha=0.5, label='All Training Errors')\n",
        "plt.hist(all_test_errors, bins=30, alpha=0.5, label='All Test Errors')\n",
        "plt.title('Overall Error Distribution (All Categories)', fontsize=12)\n",
        "plt.xlabel('Prediction Error ($)', fontsize=10)\n",
        "plt.ylabel('Frequency', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
        "plt.savefig('error_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SkH9WZlWvu_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}