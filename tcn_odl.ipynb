{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNcskBv5RMnMYlTXmu8jnn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superjoe96/LSTM/blob/main/tcn_odl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w0PnE4IX9mCv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, LayerNormalization, Activation\n",
        "from tensorflow.keras.layers import Add, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('personal_transactions_10000.csv')"
      ],
      "metadata": {
        "id": "1eRmsd1nB66x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "# Create additional temporal features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "# Convert transaction type to numeric (1 for Credit, -1 for Debit)\n",
        "df['TransactionValue'] = np.where(df['Transaction Type'] == 'credit', 1, -1)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "categorical_cols = ['Category', 'Account Name']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[f'{col}_Encoded'] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Aggregate data by week for time series forecasting\n",
        "weekly_data = df.groupby(pd.Grouper(key='Date', freq='W')).agg({\n",
        "    'Amount': 'sum',\n",
        "    'Transaction Type': 'count'  # Count of transactions per week\n",
        "}).rename(columns={'Transaction Type': 'TransactionCount'})\n",
        "\n",
        "date_range = pd.date_range(start=weekly_data.index.min(), end=weekly_data.index.max(), freq='W')\n",
        "weekly_data = weekly_data.reindex(date_range, fill_value=0)"
      ],
      "metadata": {
        "id": "aWptBMMVDNDH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 9):  # Create 8 lag features\n",
        "    weekly_data[f'Amount_Lag_{i}'] = weekly_data['Amount'].shift(i)\n",
        "\n",
        "# Create rolling statistics\n",
        "weekly_data['Amount_RollingMean_4W'] = weekly_data['Amount'].rolling(window=4).mean()\n",
        "weekly_data['Amount_RollingStd_4W'] = weekly_data['Amount'].rolling(window=4).std()\n",
        "\n",
        "# Add month and quarter as cyclical features\n",
        "weekly_data['Month'] = weekly_data.index.month\n",
        "weekly_data['Quarter'] = weekly_data.index.quarter\n",
        "\n",
        "# Create month and quarter as cyclical features\n",
        "weekly_data['Month_sin'] = np.sin(2 * np.pi * weekly_data['Month'] / 12)\n",
        "weekly_data['Month_cos'] = np.cos(2 * np.pi * weekly_data['Month'] / 12)\n",
        "weekly_data['Quarter_sin'] = np.sin(2 * np.pi * weekly_data['Quarter'] / 4)\n",
        "weekly_data['Quarter_cos'] = np.cos(2 * np.pi * weekly_data['Quarter'] / 4)\n",
        "\n",
        "# Remove rows with NaN values (from the lag and rolling features)\n",
        "weekly_data = weekly_data.dropna()\n"
      ],
      "metadata": {
        "id": "JTVn0n1WD6Ry"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the features and target\n",
        "features = [f'Amount_Lag_{i}' for i in range(1, 9)] + \\\n",
        "           ['Amount_RollingMean_4W', 'Amount_RollingStd_4W',\n",
        "            'Month_sin', 'Month_cos', 'Quarter_sin', 'Quarter_cos']\n",
        "\n",
        "target = 'Amount'\n",
        "\n",
        "# Split the data into training and testing sets (keeping the time series order)\n",
        "train_size = int(len(weekly_data) * 0.7)\n",
        "val_size = int(len(weekly_data) * 0.15)\n",
        "\n",
        "train_data = weekly_data.iloc[:train_size]\n",
        "val_data = weekly_data.iloc[train_size:train_size+val_size]\n",
        "test_data = weekly_data.iloc[train_size+val_size:]"
      ],
      "metadata": {
        "id": "mR2fO20VD7Am"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train = scaler_X.fit_transform(train_data[features])\n",
        "y_train = scaler_y.fit_transform(train_data[[target]])\n",
        "\n",
        "X_val = scaler_X.transform(val_data[features])\n",
        "y_val = scaler_y.transform(val_data[[target]])\n",
        "\n",
        "X_test = scaler_X.transform(test_data[features])\n",
        "y_test = scaler_y.transform(test_data[[target]])\n",
        "\n",
        "# Reshape input data for the TCN model [samples, timesteps, features]\n",
        "# For TCN, we reshape the input to have a single timestep with all features\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_val_reshaped = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
      ],
      "metadata": {
        "id": "XQysYsdXED9N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, dilation_rate, nb_filters, kernel_size, padding, dropout_rate=0.1):\n",
        "    \"\"\"\n",
        "    Defines the residual block for the TCN\n",
        "    \"\"\"\n",
        "    prev_x = x\n",
        "\n",
        "    # First dilated convolution\n",
        "    x = Conv1D(filters=nb_filters,\n",
        "               kernel_size=kernel_size,\n",
        "               dilation_rate=dilation_rate,\n",
        "               padding='causal',\n",
        "               activation='linear')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Second dilated convolution\n",
        "    x = Conv1D(filters=nb_filters,\n",
        "               kernel_size=kernel_size,\n",
        "               dilation_rate=dilation_rate,\n",
        "               padding='causal',\n",
        "               activation='linear')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # If the number of filters changes, use a 1x1 conv to match dimensions\n",
        "    if prev_x.shape[-1] != nb_filters:\n",
        "        prev_x = Conv1D(filters=nb_filters, kernel_size=1, padding='same')(prev_x)\n",
        "\n",
        "    # Add the residual connection\n",
        "    res = Add()([prev_x, x])\n",
        "    return res\n",
        "\n",
        "def create_tcn_model(input_shape, output_units=1):\n",
        "    \"\"\"\n",
        "    Creates an enhanced TCN model for time series prediction to address underfitting\n",
        "    \"\"\"\n",
        "    # Enhanced hyperparameters\n",
        "    nb_filters = 128  # Increased from 64 to 128 for more capacity\n",
        "    kernel_size = 3   # Increased from 2 to 3 for wider receptive field\n",
        "    nb_stacks = 2     # Increased from 1 to 2 for more depth\n",
        "    dilations = [1, 2, 4, 8, 16, 32]  # Added higher dilation rate for longer dependencies\n",
        "    dropout_rate = 0.1  # Reduced from 0.2 to prevent too much regularization when underfitting\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # TCN architecture with multiple stacks\n",
        "    x = inputs\n",
        "    for stack in range(nb_stacks):\n",
        "        for dilation_rate in dilations:\n",
        "            x = residual_block(x, dilation_rate, nb_filters, kernel_size, 'causal', dropout_rate)\n",
        "\n",
        "    # Improved output layers with additional processing\n",
        "    x = Lambda(lambda z: z[:, -1, :])(x)  # Extract last time step\n",
        "    x = Dense(64, activation='relu')(x)    # Additional dense layer for better representation\n",
        "    outputs = Dense(output_units)(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Compile with adjusted learning rate\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),  # Reduced learning rate for more stable training\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and compile the TCN model\n",
        "input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])  # (timesteps, features)\n",
        "tcn_model = create_tcn_model(input_shape)"
      ],
      "metadata": {
        "id": "iZhPtlmmEI4S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train the TCN Model\n",
        "# Define callbacks for training\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001),\n",
        "    ModelCheckpoint('tcn_cashflow_model.h5', save_best_only=True, monitor='val_loss')\n",
        "]\n",
        "\n",
        "# Re-reshape inputs to use multiple timesteps which may help the TCN learn patterns better\n",
        "# Instead of single timestep with all features, let's try using actual sequential data\n",
        "# Reshape to [samples, sequence_length, features_per_step]\n",
        "sequence_length = 8  # Using 8 weeks of historical data\n",
        "\n",
        "def create_sequences(X, y, seq_length):\n",
        "    \"\"\"Convert feature matrix to sequences for better TCN training\"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - seq_length + 1):\n",
        "        X_seq.append(X[i:i+seq_length])\n",
        "        y_seq.append(y[i+seq_length-1])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Extract the original features before reshaping\n",
        "X_train_original = X_train.copy()\n",
        "X_val_original = X_val.copy()\n",
        "X_test_original = X_test.copy()\n",
        "\n",
        "# Create sequences for training\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_original, y_train, sequence_length)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val_original, y_val, sequence_length)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_original, y_test, sequence_length)\n",
        "\n",
        "input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "tcn_model = create_tcn_model(input_shape)\n",
        "\n",
        "class_weight = None\n",
        "\n",
        "# Train the model\n",
        "history = tcn_model.fit(\n",
        "    X_train_seq,\n",
        "    y_train_seq,\n",
        "    epochs=100,  # Increased epochs to allow more time to learn\n",
        "    batch_size=32,  # Reduced batch size for better gradient estimates\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        "    class_weight=class_weight,\n",
        "    shuffle=True  # Enable shuffling to prevent order-based biases\n",
        ")"
      ],
      "metadata": {
        "id": "s2xWNvakEPde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lF6qOloqFMVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Evaluate the Enhanced TCN Model\n",
        "# Make predictions on test set\n",
        "y_pred = tcn_model.predict(X_test_seq)\n",
        "\n",
        "# Inverse transform predictions and actual values to original scale\n",
        "y_pred_original = scaler_y.inverse_transform(y_pred)\n",
        "y_test_original = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_original, y_pred_original)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
        "r2 = r2_score(y_test_original, y_pred_original)\n",
        "\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"R²: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "CjaggFokEVNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate percentage improvement compared to baseline\n",
        "# (Assuming a naive baseline that just predicts the previous value)\n",
        "y_naive = y_test_seq[:-1]  # Previous values as prediction\n",
        "y_actual = y_test_seq[1:]  # Actual values to compare against\n",
        "\n",
        "naive_mae = mean_absolute_error(scaler_y.inverse_transform(y_actual),\n",
        "                              scaler_y.inverse_transform(y_naive))\n",
        "improvement = (naive_mae - mae) / naive_mae * 100\n",
        "\n",
        "print(f\"Improvement over naive baseline: {improvement:.2f}%\")\n",
        "\n",
        "# Get the actual dates for the test set (adjusted for sequence length)\n",
        "test_dates = test_data.index[sequence_length-1:]\n",
        "if len(test_dates) > len(y_test_original):\n",
        "    test_dates = test_dates[:len(y_test_original)]\n",
        "elif len(test_dates) < len(y_test_original):\n",
        "    # This shouldn't happen, but just in case\n",
        "    y_test_original = y_test_original[:len(test_dates)]\n",
        "    y_pred_original = y_pred_original[:len(test_dates)]\n",
        "\n",
        "# Plot actual vs. predicted values with improved visualization\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(test_dates, y_test_original, label='Actual', marker='o', alpha=0.7, markersize=4)\n",
        "plt.plot(test_dates, y_pred_original, label='Predicted', marker='x', alpha=0.7, markersize=4)\n",
        "plt.title('Enhanced TCN Model: Actual vs. Predicted Weekly Cash Flow')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Amount')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add shaded area for prediction confidence interval\n",
        "plt.fill_between(test_dates,\n",
        "                y_pred_original.flatten() - mae,\n",
        "                y_pred_original.flatten() + mae,\n",
        "                alpha=0.2, color='blue',\n",
        "                label='MAE Confidence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qCNmpEOegL9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and plot error distribution with additional analysis\n",
        "errors = y_test_original - y_pred_original\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.hist(errors, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
        "plt.title('Error Distribution')\n",
        "plt.xlabel('Prediction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(y_test_original, y_pred_original, alpha=0.5, color='blue')\n",
        "plt.plot([y_test_original.min(), y_test_original.max()],\n",
        "         [y_test_original.min(), y_test_original.max()],\n",
        "         'r--', alpha=0.7)\n",
        "plt.title('Actual vs. Predicted')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(test_dates, errors, color='green', marker='o', linestyle='-', alpha=0.6, markersize=3)\n",
        "plt.title('Error Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Error (Actual - Predicted)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "from scipy import stats\n",
        "stats.probplot(errors.flatten(), plot=plt)\n",
        "plt.title('Q-Q Plot of Errors')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RG2MmAXzEaAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}